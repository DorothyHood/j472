---
title: "lab_05"
author: "Sam Carey"
date: "2023-03-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## You will need

- Tabula

## Load libraries and establish settings

```{r}
# Turn off scientific notation
options(scipen=999)

# Load the tidyverse.
library(tidyverse)
library(lubridate)
library(janitor)

```

## Get Our PDF

We'll be working with the [911 overdose calls from Baltimore County](https://drive.google.com/file/d/1qkYuojGF_6WKFr5aNQxmewDzcKyOiJFr/view?usp=share_link). You'll want to download it to a place you'll remember (like your Downloads folder, or the labs folder in your repository). The goal is to extract the tables within it, export that to a CSV file, load it into RStudio and ask some questions.

## Extract Data from PDF Using Tabula

Start Tabula, then go to http://127.0.0.1:8080/ in your browser. Click the "Browse" button and find the PDF file and click "open", and then click the "Import button" in Tabula. This will take a few seconds or longer.

This PDF has a single table spread over multiple pages to extract. We're going to make a single dataframe from this table, exporting it to a CSV file that you will load into R. In Tabula, highlight the table and click the "Preview & Export Extracted Data" button. You may want to play with including or excluding the column headers - YOU SHOULD HAVE FIVE COLUMNS OF DATA.

Save the CSV (it should be called `tabula-Baltimore County; Carey, Samantha log OD.csv` by default) to your lab_05/data folder.

From there, you will need to read in the data, and add or fix headers if necessary. You can choose to include the headers from the PDF in your exported CSV files OR to exclude them and add them when importing. `read_csv` allows us to do this ([and more](https://readr.tidyverse.org/reference/read_delim.html)).

## Load and clean up the data in R

You will need to read in and clean up the data so that it can be used for analysis. By "clean" I mean the column headers should not contain spaces and they should have meaningful names, not "x1" or something similar. How you do that is up to you, but you can use select() with or without the minus sign to include or exclude certain columns. You also can use the `rename` function to, well, rename columns. Importantly, you'll need to ensure that any columns containing a date actually have a date datatype. Our friend `lubridate` can help with this.

```{r}
baltcountcalls <- read_csv("data/tabula_Baltimore_County_Carey_Samantha_log_OD.csv", col_names = FALSE) %>%   clean_names() %>% 
  rename(date = x1, time = x2, case_number = x3, evtyp = x4, address = x5) %>% 
  mutate(date=mdy(date))
```

## Answer questions

Q1. Write code to generate the number of calls that occurred on each date. Which date in 2022 had the most overdose calls, and how many? Look at the total number of rows in your result and explore the range of dates - based on your result, do you believe there are any days with no overdose calls at all? Explain why or why not.

A1. There are two dates where the most overdose calls were made, July 14, 2022 and October 4, 2022. These days both had 23 overdose calls. From the first 10 rows of data, it seems that July, October, May, January, April, March and August all contained dates with the most amount of overdose calls. Seasonally, that seems to be more in the spring and summer than in the fall and winter. However, when scrolling through the full data sheet, it seems there were a lot of calls in the month of October, and also August, which I thought I'd note. 

```{r}
baltcountcalls %>% 
  group_by(date) %>%
  summarise(count = n()) %>% 
  arrange(desc(count))
```

Q2. You want to understand if there's a pattern in the day of the week that overdose calls are made. Add a column to your dataframe that displays what day of the week each date represents. You should search for how to do that using lubridate. Then write code to calculate the number of calls for each day of the week, and add a column to that result that calculates the percentage of all calls that occurred on each day of the week (so you want a dataframe with the day of the week, total number of calls and the percentage of calls on that day out of the total number of all calls). Describe your findings to me.

A2. I used ChatGPT to find out how to add a column that added days of the week. I used this prompt: "How can I use lubridate to add a column in my dataframe that displays the day of the week each date represents?"

The results are interesting! It seems that the day Saturday seems to have the most amount of overdose calls (638), about 15.5% of the data. Sunday is a close second (621) and about 15.1%. Thursday seems to be the day with the least amount of calls (526) and about 12.8% of the data. It's also important to note that it seems like calls were made on every day of the week Mon-Sun. The weekend days, Friday, Saturday and Sunday, all seem to have the most calls made. 

```{r}
baltcountcalls <- baltcountcalls %>%
  mutate(day_of_week = wday(date, label = TRUE)) 
  dayofweek_baltcountcalls <- baltcountcalls %>% 
    group_by(day_of_week) %>% 
    summarise(count = n()) %>% 
    arrange(desc(count)) %>% 
    mutate(percentage = count/sum(count)*100)
```

Q3. Now let's look at locations. Which ones have the most calls? How would you describe them (feel free to search for more information on them)? Is there anything about the structure of the original data that might make you less confident in the counts by location or date?

A3. At 36 calls, the location with the most calls is 4540 Silver Spring Rd. The next two are PC 02; 6424 WINDSOR MILL RD and PC 06; 115 SUSQUEHANNA AV W at 13 calls. The location on Silver Spring Rd is in the northeast area of Baltimore. It seems to be right off of the main road and covered behind shrubbery (according to pictures from Zillow and Redfin). It was sold for $100,000 on Jan. 18, 2023. It's interesting because the Zillow and Redfin estimates are much more than that sold amount. PC 02; 6424 WINDSOR MILL RD and PC 06; 115 SUSQUEHANNA AV W are both police precincts, which is interesting. One is more in the northwest of Baltimore and the other more directly up north. 

One thing about the original data that seems off is there are some repeats of dates, times and locations multiple times throuhgout the data. Two examples are 1015 DONINGTON CI, shown twice at the same time (15:52:52) and same date (2022-02-06) and 6101 GENTRY LA, listed three times with the same time (20:50:38) and the same date (2022-02-07). This can cause some questioning, as in, what do these duplicates represent? Were there two calls or three calls made at the same time on the same day for the same location? Were there two or three overdoses at that address on the same day and at the same time? Was it a mistake on the log part and it wasn't meant to be put in more than once? This then takes away from showing the correct and accurate totals of 911 calls. 

```{r}
baltcountcalls %>% 
  group_by(address) %>%
  summarise(count = n()) %>% 
  arrange(desc(count))
```

Q4. What's the best story idea or question you've seen as a result of the work you've done in this lab?

A4. What region of Baltimore County (north, south, etc.) experienced the most overdose calls in 2022 and why? This could bring forth questions of drug distribution throughout the county. What demographics, socio-economic statuses, etc. are dominant in those areas? 
